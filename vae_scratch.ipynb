{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.07600354062600563, 0.0, 0.0, 0.013553286621692723, 0.0, 0.0, 0.0, 0.036905342161933696, 0.0, 0.0]\n",
      "Mean: [0.08191093588850454, 0.014311286142674769]\n",
      "Log Variance: [0.04863111710276943, 0.1228066165677772]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return sum(x * y for x, y in zip(v1, v2))\n",
    "\n",
    "\n",
    "def matrix_vector_mult(mat, vec):\n",
    "    return [dot_product(row, vec) for row in mat]\n",
    "\n",
    "\n",
    "def vector_add(v1, v2):\n",
    "    return [x + y for x, y in zip(v1, v2)]\n",
    "\n",
    "\n",
    "def sample_normal(mean, std_dev):\n",
    "    return mean + std_dev * random.gauss(0, 1)\n",
    "\n",
    "\n",
    "class SimpleLinear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = [[random.normalvariate(0, 0.1) for _ in range(\n",
    "            input_dim)] for _ in range(output_dim)]\n",
    "        self.bias = [random.normalvariate(0, 0.1) for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return vector_add(matrix_vector_mult(self.weights, x), self.bias)\n",
    "\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.encoder = SimpleLinear(input_dim, latent_dim)\n",
    "        self.mean_layer = SimpleLinear(latent_dim, 2)\n",
    "        self.logvar_layer = SimpleLinear(latent_dim, 2)\n",
    "        self.decoder = SimpleLinear(2, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoded = [relu(y) for y in self.encoder.forward(x)]\n",
    "        mean = self.mean_layer.forward(encoded)\n",
    "        logvar = self.logvar_layer.forward(encoded)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        std_dev = [2 ** (0.5 * lv) for lv in logvar]\n",
    "        return [sample_normal(m, sd) for m, sd in zip(mean, std_dev)]\n",
    "\n",
    "    def decode(self, z):\n",
    "        return [relu(y) for y in self.decoder.forward(z)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "vae = VAE(input_dim=100, latent_dim=5)\n",
    "input_vector = [random.random() for _ in range(10)]\n",
    "output, mean, logvar = vae.forward(input_vector)\n",
    "print(\"Output:\", output)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Log Variance:\", logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Latent Vector: [-0.8376461546995724]\n",
      "Decoded Output: [0.0, 0.0, 0.19070853738246202, 0.0, 0.0, 0.0, 0.0, 0.04095718092015972, 0.02483993027561232, 0.0, 0.041146695015348796, 0.09549244310231193, 0.0, 0.1239106873465789, 0.03855543294124896, 0.0, 0.0, 0.0, 0.0, 0.17358647174616132, 0.012693051904520417, 0.0, 0.2139588840317164, 0.0, 0.06703320151612105, 0.11962746115542884, 0.07231748717717223, 0.0, 0.2533115864099085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07766497666418488, 0.18338525027693353, 0.0, 0.22782076072837432, 0.018941834409103617, 0.0, 0.0, 0.0, 0.0331015912649107, 0.0, 0.13060016989984957, 0.0, 0.0, 0.04965037414502439, 0.0, 0.13026039498430297, 0.17734919672910832, 0.08521975705459643, 0.05684230448927546, 0.014489481410471017, 0.0, 0.0, 0.0, 0.0, 0.04014395440029998, 0.0, 0.15672668276599827, 0.0, 0.0, 0.14865771011123532, 0.049189500126404856, 0.14719746552887064, 0.048985343485688795, 0.0, 0.16224178862382768, 0.1276479681621836, 0.0, 0.0, 0.16530342175992968, 0.15821619374621443, 0.10333875491189305, 0.0, 0.0, 0.0, 0.0, 0.24584557110032107, 0.0, 0.16530930676900013, 0.023388861877392256, 0.1264730696286725, 0.0, 0.07292179090395301, 0.008681982962073061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16324231922923035, 0.11655217281552865, 0.0, 0.0, 0.0, 0.08490233459236358, 0.0, 0.023909601826001596]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return sum(x * y for x, y in zip(v1, v2))\n",
    "\n",
    "\n",
    "def matrix_vector_mult(mat, vec):\n",
    "    return [dot_product(row, vec) for row in mat]\n",
    "\n",
    "\n",
    "def vector_add(v1, v2):\n",
    "    return [x + y for x, y in zip(v1, v2)]\n",
    "\n",
    "\n",
    "def sample_normal(mean, std_dev):\n",
    "    return mean + std_dev * random.gauss(0, 1)\n",
    "\n",
    "\n",
    "class SimpleLinear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = [[random.normalvariate(0, 0.1) for _ in range(\n",
    "            input_dim)] for _ in range(output_dim)]\n",
    "        self.bias = [random.normalvariate(0, 0.1) for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return vector_add(matrix_vector_mult(self.weights, x), self.bias)\n",
    "\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.encoder = SimpleLinear(input_dim, latent_dim)\n",
    "        self.mean_layer = SimpleLinear(latent_dim, 2)\n",
    "        self.logvar_layer = SimpleLinear(latent_dim, 2)\n",
    "        self.decoder = SimpleLinear(2, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoded = [relu(y) for y in self.encoder.forward(x)]\n",
    "        mean = self.mean_layer.forward(encoded)\n",
    "        logvar = self.logvar_layer.forward(encoded)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        std_dev = [2 ** (0.5 * lv) for lv in logvar]\n",
    "        return [sample_normal(m, sd) for m, sd in zip(mean, std_dev)]\n",
    "\n",
    "    def decode(self, z):\n",
    "        return [relu(y) for y in self.decoder.forward(z)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "\n",
    "# Manually set the mean and variance\n",
    "manual_mean = 0.5\n",
    "manual_variance = 0.1\n",
    "vae = VAE(input_dim=100, latent_dim=5)\n",
    "# Sample from latent space using reparameterization\n",
    "z_sample = vae.reparameterization([manual_mean], [manual_variance])\n",
    "\n",
    "# Decode the sampled latent vector\n",
    "decoded_output = vae.decode(z_sample)\n",
    "\n",
    "print(\"Sampled Latent Vector:\", z_sample)\n",
    "print(\"Decoded Output:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-python-3-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
